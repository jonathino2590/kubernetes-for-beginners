* Common
kubectl get pods --all-namespaces -o=jsonpath='{range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{", "}{end}{end}' |sort
kubectl get componentstatuses
#recursivida en archivos 
k apply -R path/
k describe -f file.yaml
k diff -f path
k top pod --use-protocol-buffers #ver las metricas de CPy y MEMORY 
k top nodes --use-protocol-buffers #ver las metricas de CPy y MEMORY
watch "kubectl top pod --use-protocol-buffers" 
k logs -f name-pod name-container # ver los registros de un pod cuando se tiene mas de un contenedor se coloca el nombre del mismo 
k logs -f name-pod -c name-container # ver los registros de un pod
k get pods -n kube-system #pod del system de k8s
k get pods -l app=myapp --no-headers  #seleccionar con etiquetas sin cabecera
k get pods -l app=myapp,env=prod --no-headers  #seleccionar con etiquetas sin cabecera
k cp directorarchivos-6689dc459f-k6rn6:/opt/directorArchivos/directorArchivos.jar ./prueba.jar //copiar un archivo de un pod

for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done


#pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)

k explain pod --recursive | grep -A5 tolerations

k version --short


* Deployments 
Almacena una notacion de apply
kubectl create deploy nginx --image=nginx -r=3  --dry-run=client --save-config --show-managed-fields -o yaml

kubectl create deployment --image=nginx nginx
# deployment con replicas
kubectl create deployment nginx --image=nginx --replicas=4
kubectl create deployment nginx --image=nginx -r=4

k delete deploy --all #borrar todos los deploy 

para eliminacion de deploy 

k delete deploy demo-nginx --grace-period=0 --force

* Pods
kubectl run NAME --image=image [--env="key=value"] [--port=port] [--dry-run=server|client] [--overrides=inline-json] [--command] -- [COMMAND] [args...]
Crear un pod NGINX
kubectl run nginx --image=nginx

crear un pods con labels 
k run redis --image=redis:alpine --labels="tier=db" --dry-run=client -o yaml

Genere el archivo YAML de manifiesto POD (-o yaml). No lo cree (–secar-ejecutar)
kubectl run nginx --image=nginx --dry-run=client -o yaml

crear un pod y exponerlo (crea un pod y service )
kubectl run httpd --image=httpd:alpine --port 80 --expose --dry-run=client -o yaml

#editar un pod en ejecucion 
k edit pod name-pod 
esto genera un archivo en /tmp
k replace -f /tmp/kubectl-edit-xxx.yaml --force 

O
eliminar el pod y luego hacer el create o apply , otra manera es editar el kubectl get pod webapp -o yaml > my-new-pod.yaml el archivo y asi aplicarlo 

#scalar
k scale deploy nginx -r=5
k scale deployment nginx --rreplicas=5

#actualizar una imagen
k set image deployment nginx nginx=nginx:1.18

pod estaticos 
crearalo en la carpeta /etc/kubernetes/manifiest o verificar la ruta en el kubelet.service o /var/lib/kubelet/config.yaml 
lo podemos ver con el comando systemctl status kubelet 

k run static-busybox --image=busybox --command spleep 1000  --dry-run=client

* Svc
#exponer un servicio Node port
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml # tcp = port:targetPort
kubectl expose deployment name-deploy --name=webapp-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml > sv.yaml

Cree un servicio llamado redis-service de tipo ClusterIP para exponer pod redis en el puerto 6379
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml (Esto usará automáticamente las etiquetas del pod como selectores)
kubectl expose pod redis --port 6379 --name redis-service --dry-run=client -o yaml
O
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
(Esto no utilizará las etiquetas de los pods como selectores, sino que asumirá los selectores como app = redis.
 No puede pasar los selectores como una opción. Por lo tanto, no funciona muy bien si su pod tiene un conjunto de etiquetas diferente. 
 Por lo tanto, genere el archivo y modificar los selectores antes de crear el servicio)


 * HPA 
 kubectl hpa (-f FILENAME | TYPE NAME | TYPE/NAME) [--min=MINPODS] --max=MAXPODS [--cpu-percent=CPU]

 kubectl autoscale deployment nginx --min=2 --max=10 --cpu-percent=80
 kubectl hpa deployment nginx --min=2 --max=10 --cpu-percent=80

*hablitar addons minikube 
minikube addons enable ingress 
minikube addons enable metrics-server #otras git clone git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git https://github.com/kubernetes-incubator/metrics-server.git

*Binding 
curl --header "Content-Type:application/json" --request POST --data 'JSON DEL BINDING'
http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/

Taints & Telerations

 # taint-effect 
 #NoSchedule / No se programara 
 #PreferNoSchedule / No se colocara ninguna pieza en ese nodo no esta garantizado 
 #NoExecute / no se ejecutara
 #k taint nodes node-name key=value:taint-effect
 #agregar
 kubectl taint nodes minikube app=blue:NoSchedule
 #delete
 kubectl taint nodes minikube app-



JSON PATH 


formats are: custom-columns,custom-columns-file,go-template,go-template-file,json,jsonpath,jsonpath-as-json,jsonpath-file,name,template,templatefile,wide,yaml
k get pods -n kube-system -o=jsonpath='{.items[*].metadata.name}{"\n"}{.items[*].metadata.labels}'
k get pods -n kube-system -o=jsonpath='{.items[0].spec.containers[*].image}'
k get pods -o=custom-columns=POD:.metadata.name,CPU:status.capacity
k get pods,svc -o=custom-columns=POD:.metadata.name
k get nodes --sort-by= .metadata.name
kubectl get -o template pod myapp-pod --template={{.status.phase}}
k get pods -n kube-system -o=jsonpath='{.items[*].metadata.name} {"\t"} {.items[*].spec.containers[*].image}'

loops
k get pods -n kube-system -o=jsonpath='{.items[*].metadata.name} {"\n"} {.items[*].spec.containers[*].image}'
k get pods -n kube-system -o=jsonpath='{range .items[*]} {.metadata.name}{"\t"}{.spec.containers[*].image}{"\t"}{end}'

$.status.containerStatuses[?(@.name == 'redis-container')].restartCount



stress de pod 

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2021-08-26T04:32:48Z"
  name: elephant
  namespace: default
  resourceVersion: "1711"
  uid: f29b5949-3bd3-451d-9a95-c06759ea9851
spec:
  containers:
  - args:
    - --vm
    - "1"
    - --vm-bytes
    - 15M
    - --vm-hang
    - "1"
    command:
    - stress
    image: polinux/stress
    imagePullPolicy: Always
    name: mem-stress
    resources:
      limits:
        memory: 20Mi
      requests:
        memory: 5Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-wtn9f
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: default-token-wtn9f
    secret:
      defaultMode: 420
      secretName: default-token-wtn9f


*ConfigMaps
k create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MOD=prod
k create configmap app-config --from-file=<path-to-file> # esto se crea con el nombre del archivo la data

archivo.yaml
APP_COLOR: blue
APP_MODE: prod


apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: default
data:
  configmaps-file.yaml: |
    APP_COLOR: blue
    APP_MODE: prod

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - envFrom:
    - configMapRef:
         name: webapp-config-map
    image: kodekloud/webapp-color
    name: webapp-color    

*Secret
k create secret generic app-secret --from-literal=APP_COLOR=blue --from-literal=APP_MOD=prod
k create secret generic app-secret --from-env-file=path/to/bar.env 
k create secret generic app-secret --from-file=<path-to-file> # esto se crea con el nombre del archivo la data
echo -n palabra | base64
echo -n palabra | base64 --decode


--kube-controller-manager
-pod-eviction-timeout=5m0S #tiempo que espera el master a que un nodo este en linea antes de declararlo muerto

#drenar un nodo
k cordon node-1 #coloca al nodo como no programable 
k drain node-1 # drenas los pods de ese nodo a otros nodos --ignore-daemonsets para ignorar varios daemon
# solo drena los rs, rs ,ds ss, job si algo q no forma parte de estos kind debe usar --force
k uncordon node-1 #coloca el nodo como programable

actualizar un cluster master con kubeadmin
kubeadm upgrade plan
kubeadm upgrade apply 

primero se debe actualizar el kubeadm actualizacion de master
apt-get upgrade -y kubeadm=1.12.0-00
kubeadm upgrade apply v1.12.0
apt-get upgrade -y kubelet=1.12.0-00
systemctl restart kubelet 

On the controlplane node, run the command run the following commands:

apt update
This will update the package lists from the software repository.

apt install kubeadm=1.20.0-00 
This will install the kubeadm version 1.20

kubeadm upgrade apply v1.20.0
This will upgrade kubernetes controlplane. Note that this can take a few minutes.

apt install kubelet=1.20.0-00 This will update the kubelet with the version 1.20.
o 
apt-get upgrade kubelet
You may need to restart kubelet after it has been upgraded.
Run: systemctl restart kubelet


actualizacion de node 
k cordon node-1
k drain node-1 # drenado de nodo el drain hace el cordon automatico
apt-get upgrade -y kubeadm=1.20.0-00
apt-get upgrade -y kubelet=1.12.0-00
kubeadm upgrade node config --kubelet-version v1.20.0
systemctl restart kubelet 

On the node01 node, run the command run the following commands:

If you are on the master node, run ssh node01 to go to node01


apt update
#This will update the package lists from the software repository.


apt install kubeadm=1.20.0-00
#This will install the kubeadm version 1.20


kubeadm upgrade node
#This will upgrade the node01 configuration.


apt install kubelet=1.20.0-00 This will update the kubelet with the version 1.20.


You may need to restart kubelet after it has been upgraded.
Run: systemctl restart kubelet

#ver los config maps 
kubectl -n kube-system get cm kubeadm-config -oyaml

#actualizar el nodo de minikube 
curl -Lo minikube https://storage.googleapis.com/minikube/releases/v1.23.0/minikube-linux-amd64 && chmod +x minikube && sudo cp minikube /usr/local/bin/ && rm minikube
minikube delete 
minikube start 

*********************backup ETCD***************

verificacion de resplado 

ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 o 127.0.0.1:2379  \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
member list 


Use the etcdctl snapshot save command. You will have to make use of additional flags to connect to the ETCD server.
--endpoints: Optional Flag, points to the address where ETCD is running (127.0.0.1:2379)
--cacert: Mandatory Flag (Absolute Path to the CA certificate file)
--cert: Mandatory Flag (Absolute Path to the Server certificate file)
--key:Mandatory Flag (Absolute Path to the Key file)

ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db

#status
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot status /opt/snapshot-pre-boot.db #--listar

--restaurar
First Restore the snapshot:

root@controlplane:~# ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db


2021-03-25 23:52:59.608547 I | mvcc: restore compact to 6466
2021-03-25 23:52:59.621400 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32
root@controlplane:~# 
Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, 
the only required option for the restore command is the --data-dir.

cuando es de otro server la restauracion debe hacerse de la siguiente manera

ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
--initial-cluster="minikube=https://127.0.0.1:2380" \
# el nombre sale del miembro del etcd --name=minikube del pod
--name="minikube" \
--initial-advertise-peer-urls="http://127.0.0.1:2380" \
 #debe darse un nombre unico para el cluster
 --initial-cluster-token="etcd-cluster-1" \

--data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db

Next, update the /etc/kubernetes/manifests/etcd.yaml:

We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the 
volume called etcd-data from old directory (/var/lib/etcd) to the new directory /var/lib/etcd-from-backup.

cambiar en el pod estatico 

- --data-dir=/var/lib/etcd-from-backup
 # se le agrega 
- --initial-cluster-token=etcd-cluster-1

  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
With this change, /var/lib/etcd on the container points to /var/lib/etcd-from-backup on the controlplane (which is what we want)

When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the /etc/kubernetes/manifests directory.

verificar si se creo otro miembro 

ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 o 127.0.0.1:2379  \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
member list 


service kube-api-server stop
#se debe carmbiar el systemctl de etcd o el manifiesto para q apunte a la nueva ruta del etcd
sytemctl daemon-reload
service etcd restart


 
#ver la version 
kubectl -n kube-system logs etcd-controlplane | grep -i 'etcd Version'
2021-03-25 22:26:52.984979 I | etcdmain: etcd Version: 3.4.13
kubectl -n kube-system describe pod etcd-controlplane | grep Image:
Image:         k8s.gcr.io/etcd:3.4.13-0

#cerificado
kubectl -n kube-system describe pod etcd-controlplane | grep '\--cert-file'

******redis cluster**********
helm repo add bitnami https://charts.bitnami.com/bitnami

helm pull bitnami/redis-cluster --untar

helm install redis-cluster-master redis-cluster --set service.name=jonathan-redis --dry-run > redis-cluster/redis-cluster4.yaml